#!/usr/bin/env python
import argparse
import json
import logging
import math
import numpy as np
import os
import re
import requests
import sys
import time
import warnings

RE_URL = re.compile(r'^([\w+]+:)')

class Dataset:
  """
  Internal representation of the raw values of a dataset
  """

  def __init__(self, contents):
    self.config = contents.get('config', None)
    self.raw = contents['raw']
    self.axes = list(self.raw[0]['parameters'].keys())
    self.metrics = sorted(list(self.raw[0]['values'].keys()))
    self.meta = contents.get('meta', {})

  def dataPairs(self, axis, metric):
    """
    Collect all the interesting data pairs
    """
    pairs = []
    for sample in self.raw:
      xv = sample['parameters'][axis]
      for value in sample['values'][metric]:
        pairs.append(
          (xv, value[1])
        )
    return np.array(pairs)

  def metricConfig(self, metric, default=None):
    """
    Lookup metric configuration until we find the one we need
    """
    if not self.config:
      return default
    if not 'config' in self.config:
      return default
    if not 'metrics' in self.config['config']:
      return default

    # Return found conig
    for conf in self.config['config']['metrics']:
      if conf['name'] == metric:
        return conf
    return default

  def axisConfig(self, metric, default=None):
    """
    Lookup metric configuration until we find the one we need
    """
    if not self.config:
      return default
    if not 'config' in self.config:
      return default
    if not 'parameters' in self.config['config']:
      return default

    # Return found conig
    for conf in self.config['config']['parameters']:
      if conf['name'] == metric:
        return conf
    return default

  def stripMetric(self, name):
    """
    Remove this metric from the dataset
    """
    if name in self.metrics:
      i = self.metrics.index(name)
      del self.metrics[i]
      for r in self.raw:
        r['values'][name]

def pf_plotDiff(axis, summ1, summ2, points1, points2, maxis, mmetric):
  """
  Difference between the two plots
  """
  y_diff = summ2[:,1] - summ1[:,1]
  y_delta = np.sqrt( summ1[:,2] ** 2 / summ1[:,3] + summ2[:,2] ** 2 / summ2[:,3] )
  with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    y_z = np.nan_to_num(y_diff / y_delta)
  # axis.errorbar(summ2[:,0], y_diff, yerr=y_delta, label="diff")
  axis.set_ylabel('Sigmas')
  p1 = axis.plot(summ2[:,0], y_z)
  p2 = axis.plot([0,np.max(summ2[:,0])], [0, 0], c='black')
  axis.set_title(mmetric['name'])
  return ((p1[0], p2[0]), ('Difference (sigmas)', 'Zero'))

def pf_plotSumm(axis, summ1, summ2, points1, points2,  maxis, mmetric):
  """
  Metric configuration
  """
  c1 = '#14c684'
  c2 = '#f9a328'
  axis.grid(b=True, color='#dadde2', linestyle='dotted')
  l1 = axis.plot(summ1[:,0], summ1[:,1], '-', linewidth=2, color=c1)
  l2 = axis.plot(summ2[:,0], summ2[:,1], '-', linewidth=2, color=c2)
  p1 = axis.errorbar(summ1[:,0], summ1[:,1], yerr=summ1[:,2], capsize=5, fmt='.',
    mfc=l1[0].get_color(), color=l1[0].get_color(), ecolor=l1[0].get_color())
  p2 = axis.errorbar(summ2[:,0], summ2[:,1], yerr=summ2[:,2], capsize=5, fmt='.',
    mfc=l2[0].get_color(), color=l2[0].get_color(), ecolor=l2[0].get_color())
  axis.set_ylabel(mmetric['units'])
  axis.set_title(mmetric['name'])
  axis.margins(0.05, 0.1)
  axis.patch.set_visible(False)
  return ((l1[0], l2[0]), ('Input', 'Reference'))

def pf_plotScatter(axis, summ1, summ2, points1, points2,  maxis, mmetric):
  """
  Metric configuration
  """
  c1 = '#14c684'
  c2 = '#f9a328'
  axis.grid(b=True, color='#dadde2', linestyle='dotted')
  # l1 = axis.plot(summ1[:,0], summ1[:,1], '-', linewidth=2, color=c1)
  # l2 = axis.plot(summ2[:,0], summ2[:,1], '-', linewidth=2, color=c2)
  # p1 = axis.errorbar(summ1[:,0], summ1[:,1], yerr=summ1[:,2], capsize=5, fmt='.',
  #   mfc=l1[0].get_color(), color=l1[0].get_color(), ecolor=l1[0].get_color())
  # p2 = axis.errorbar(summ2[:,0], summ2[:,1], yerr=summ2[:,2], capsize=5, fmt='.',
  #   mfc=l2[0].get_color(), color=l2[0].get_color(), ecolor=l2[0].get_color())

  # Calculate a horizontal spread
  x1 = np.arange(0,len(points1[:,0])*2,2)
  x2 = np.arange(1,len(points2[:,0])*2+1,2)

  p2 = axis.scatter(x2, points2[:,1], s=2, color=c2)
  p1 = axis.scatter(x1, points1[:,1], s=2, color=c1)

  axis.set_ylabel(mmetric['units'])
  axis.set_title(mmetric['name'])
  axis.margins(0.05, 0.1)
  axis.patch.set_visible(False)
  return ((p1, p2), ('Input', 'Reference'))

def pf_plotQQ(axis, summ1, summ2, points1, points2, maxis, mmetric):
  """
  Metric configuration
  """
  p1 = axis.scatter(summ1[:,1], summ2[:,1])
  lb = np.min(summ1[:,1])
  ub = np.max(summ1[:,1])
  axis.set_ylabel('{} ({})'.format(metric['name'], metric['units']))
  axis.set_xlabel('{} ({})'.format(metric['name'], metric['units']))
  axis.plot([lb, ub], [lb, ub])
  axis.set_title(mmetric['name'])
  return ((p1,), ('Input - Reference',))

def calcDiffSD(summ1, summ2):
  """
  Calculate the standard deviation of the difference of the samples
  """
  y_diff = np.abs(summ2[:,1] - summ1[:,1])
  y_sd_by_n = np.sqrt( summ1[:,2] ** 2 / summ1[:,3] + summ2[:,2] ** 2 / summ2[:,3] )

  # Calculate the Z-score
  with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    z_score = np.nan_to_num(y_diff / y_sd_by_n)

  # Return the difference in sigmas
  return np.sum(z_score) / len(z_score)

def calcDiffOfMetrics(datasets1, datasets2, axis, metrics):
  """
  Calculate the difference of every metric
  """
  ans = {}

  # Combine the metrics of all datasets
  for metric in metrics:
    points1 = rejectOutliers(normalizeAndMergeDatapoint(
      list(map(lambda d: d.dataPairs(axis, metric), datasets1))
    ), 4.0)
    points2 = rejectOutliers(normalizeAndMergeDatapoint(
      list(map(lambda d: d.dataPairs(axis, metric), datasets2))
    ), 4.0)

    # Normalize data points
    (points1, points2) = normalizeOverlappingDatapoints(points1, points2)

    # Summarize observations
    summ1 = summarizeDatapoints(points1)
    summ2 = summarizeDatapoints(points2)

    ans[metric] = calcDiffSD(summ1, summ2)

  return ans

def displayPlot(datasets1, datasets2, axis, metrics, plotFn, plotFile):
  """
  Plots something
  """
  import matplotlib.pyplot as plt
  from matplotlib.ticker import NullFormatter  # useful for `logit` scale

  usingQt4 = False
  if plotFile is None:
    try:
      plt.switch_backend('Qt4Agg')
    except ImportError:
      usingQt4 = False

  # Generate plot figures
  row_plots = 2
  y = math.ceil(len(metrics) / row_plots)
  x = min(len(metrics), row_plots)
  fig, axarr = plt.subplots(x, y, facecolor='#FFFFFF', figsize=(20, 10), dpi=72)
  x_i = 0
  y_i = 0

  # Used for legend plotting
  first_lines = None

  # Get axis meta-data
  meta_axis = datasets1[0].axisConfig(axis, {"name": axis, "units": ""})

  # In case we have only one plot axarr will become scalar value
  if x == 1 and y == 1:
    axarr = np.array([np.array([axarr])])
  elif y == 1:
    axarr = np.array([np.array(axarr)]).transpose()

  # Combine the metrics of all datasets
  for metric in metrics:
    points1 = rejectOutliers(normalizeAndMergeDatapoint(
      list(map(lambda d: d.dataPairs(axis, metric), datasets1))
    ), 4.0)
    points2 = rejectOutliers(normalizeAndMergeDatapoint(
      list(map(lambda d: d.dataPairs(axis, metric), datasets2))
    ), 4.0)

    # Summarize observations
    summ1 = summarizeDatapoints(points1)
    summ2 = summarizeDatapoints(points2)

    # Extract metric meta-data
    meta_metric = datasets1[0].metricConfig(metric, {"name": metric, "units": ""})

    # Call the plot function on the axis
    lines = plotFn(axarr[x_i, y_i], summ1, summ2, points1, points2, meta_axis, meta_metric)
    if first_lines is None:
      first_lines = lines

    # Advance to next plot
    x_i += 1
    if x_i >= x:
      x_i = 0
      y_i += 1

  # Show legend
  fig.legend(first_lines[0], first_lines[1], 'lower right')
  plt.gca().yaxis.set_minor_formatter(NullFormatter())
  plt.subplots_adjust(top=0.95, bottom=0.12, left=0.04, right=0.98, hspace=0.25,
                      wspace=0.25)

  if usingQt4:
    figM = plt.get_current_fig_manager()
    figM.window.showMaximized()

  if plotFile is None:
    plt.show()
  else:
    plt.savefig(plotFile, dpi=144)


def normalizeAndMergeDatapoint(pairList):
  """
  Compute the common overlapping set of points in the given list of pairs and
  merge them together to increase the statistical accuracy.
  """
  ans = pairList[0]
  for i in range(1, len(pairList)):
    (ans, new) = normalizeOverlappingDatapoints(ans, pairList[i])
    ans = np.concatenate([ans, new])

  return ans

def normalizeOverlappingDatapoints(pairs1, pairs2):
  """
  Processes the two data pairs and returns the common subset of data points.
  If some data points are missing, this function will do a linear interpolation
  between the neighboring two points.
  """

  dp1_x = np.unique(pairs1[:,0])
  dp2_x = np.unique(pairs2[:,0])

  # Get the maximum overlapping data bounds since we don't want to
  # extrapolate data. We only interpolate.
  min_x = max(np.min(dp1_x), np.min(dp2_x))
  max_x = min(np.max(dp1_x), np.max(dp2_x))

  # Filter-out results
  dp1_x = dp1_x[np.logical_and(dp1_x >= min_x, dp1_x <= max_x)]
  dp2_x = dp2_x[np.logical_and(dp2_x >= min_x, dp2_x <= max_x)]

  # Check which missing x values we have in either of datasets
  missing_1 = []
  missing_2 = []
  for i in dp1_x:
    if not i in dp2_x:
      missing_1.append(i)
  for i in dp2_x:
    if not i in dp1_x:
      missing_2.append(i)

  # Interpolate data points on x
  interp1 = np.array(list(zip(missing_1, np.interp(missing_1, pairs1[:,0], pairs1[:,1]))))
  interp2 = np.array(list(zip(missing_2, np.interp(missing_2, pairs2[:,0], pairs2[:,1]))))

  # Make sure shape is correct on empty arrays
  if not len(interp1):
    interp1.shape = (0,2)
  if not len(interp2):
    interp2.shape = (0,2)

  # Filter and combine results
  return (
    np.concatenate([interp1, list(filter(lambda xy: xy[0] >= min_x and xy[0] <= max_x, pairs1))]),
    np.concatenate([interp2, list(filter(lambda xy: xy[0] >= min_x and xy[0] <= max_x, pairs2))])
  )

def rejectOutliers(pairs, m = 2.):
  """
  Reject outliers from the given sample
  """
  d = np.abs(pairs[:,1] - np.median(pairs[:,1]))
  mdev = np.median(d)
  s = d/mdev if mdev else np.zeros(len(pairs))
  return pairs[s<m]

def summarizeDatapoints(pairs):
  """
  Groups the data points on x values and calculates standard error of mean when
  more than one data point is collected per bin
  """

  # Create bins
  bins = np.unique(pairs[:,0])
  bin_values = [pairs[pairs[:,0] == x] for x in bins]

  # Calculate mean and sd
  bin_count = [len(values) for values in bin_values]

  bin_mean = [np.mean(values[:,1]) for values in bin_values]
  bin_sd = [np.std(values[:,1]) for values in bin_values]

  bin_err = np.array(bin_sd) * 1.96 / np.sqrt(np.array(bin_count))

  # Compose and return
  return np.array(
    list(zip(bins, bin_mean, bin_sd, bin_count))
  )

def loadDataset(uri, ignore_metric=[]):
  """
  Load dataset either from file or from the web
  """

  # If this doesn't look like a URL, assume it's a file
  if not RE_URL.match(uri):
    uri = 'file:' + uri

  # Handle URLs
  (scheme, path) = uri.split(':', 1)
  scheme = scheme.lower()

  # Web scheme
  if scheme in ('http', 'https', 'ftp'):
    res = requests.get(uri)
    if res.status_code < 200 or res.status_code >= 300:
      raise IOError(
        'Received an unexpected HTTP {} response while fetching {}'.format(
          res.status_code, uri))
    dataset = Dataset(json.loads(res.text))

  # File scheme
  elif scheme in ('file',):
    if not os.path.isfile(path):
      raise IOError('Could not find file {}'.format(path))
    with open(path, 'r') as f:
      contents = f.read()
      dataset = Dataset(json.loads(contents))

  # Unknown scheme
  else:
    raise IOError('Unknown scheme "{}" in URI {}'.format(scheme, uri))

  # Remove ignored metrics
  if ignore_metric:
    for metric in ignore_metric:
      dataset.stripMetric(metric)

  return dataset


if __name__ == '__main__':
  """
  Entry point
  """

  # Parse arguments
  parser = argparse.ArgumentParser(description='The DC/OS Performance Comparison Tool')

  parser.add_argument('-i', dest='input', action='append',
                      help='one or more paths (or URLs) to the input raw dump')
  parser.add_argument('-r', dest='ref', action='append',
                      help='one or more paths (or URLs) to the reference raw dump')

  parser.add_argument('--ignore-metric', dest='ignore_metric', action='append',
                      help='one or more metrics to ignore when processing')

  parser.add_argument('-v', '--verbose', action='store_true', dest='verbose',
                      help='Enable verbose logging')
  parser.add_argument('-s', '--silent', action='store_true', dest='silent',
                      help='Disable all output')

  parser.add_argument('-P', '--plot', action='store_true', dest='plot',
                      help='View results as plots')

  parser.add_argument('--plot-file', default=None, type=str, dest='plot_file',
                      help='Dump the plot to the given file instead of displaying')

  parser.add_argument('-w', '--weight', action='append', dest='weights',
                      help='Specify metric contribution between 1.0 (full) and 0.0 (none)')

  parser.add_argument('-t', '--threshold', default=1.0, type=float, dest='threshold',
                      help='The threshold (in standard deviations) to accept')

  parser.add_argument('-W', '--weight-only', action='store_true', dest='weight_only',
                      help='If specified, all metrics other than the ones defined by -w will have weight 0.0 instead of 1.0')

  parser.add_argument('--json', action='store_true', dest='json',
                      help='Return results in JSON format')

  args = parser.parse_args()

  # Setup logging
  level = logging.INFO
  if args.verbose:
    level = logging.DEBUG
  if args.silent:
    level = logging.CRITICAL
  logging.basicConfig(format='%(asctime)s: %(message)s', level=level)
  logger = logging.getLogger('dcostool')

  # Expand argument weights
  weights = {}
  inv_weights = {}
  if args.weights:
    for arg in args.weights:
      if not '=' in arg:
        raise ValueError('Please specify weights in `metric=weight` format')
      (name, score) = arg.split('=', 1)
      weights[name] = float(score)

  # Load datasets
  dataset_input = []
  dataset_reference = []
  for fname in args.input:
    try:
      dataset_input.append(loadDataset(fname, args.ignore_metric))
    except Exception as e:
      logger.error('{} while loading {}: {}'.format(type(e).__name__, fname, e))
      sys.exit(1)
  for fname in args.ref:
    try:
      dataset_reference.append(loadDataset(fname, args.ignore_metric))
    except Exception as e:
      logger.error('{} while loading {}: {}'.format(type(e).__name__, fname, e))
      sys.exit(1)

  # Print the Z-score for all metrics
  scores = calcDiffOfMetrics(
    dataset_input,
    dataset_reference,
    dataset_input[0].axes[0],
    dataset_input[0].metrics
  )

  if not args.json:
    print("{:30s} {:10s} {:10s}   {:10s}".format("Metric", "Z-Score", "Weight", "Weighted"))
    print("----------------------------------------------------------------")

  # Calculate overall score
  value = 0
  total = 0
  for name, score in scores.items():
    weight = weights.get(name, 0.0 if args.weight_only else 1.0)
    weighted_score = 0
    accounted = " "
    if not math.isnan(score):
      weighted_score = score * weight#
    if weighted_score != 0:
      accounted = "+"
      value += weighted_score
      total += weight

    if not args.json:
      print("{:30s} {:<10.3f} {:<10.2f} {} {:<10.3f}".format(name, score, weight, accounted, weighted_score))

  if total == 0:
    value = 0
  else:
    value /= total

  if not args.json:

    if value > args.threshold:
      status = "REJECTED (threshold = {:.1f} σ)".format(args.threshold)
    else:
      status = "Accepted (threshold = {:.1f} σ)".format(args.threshold)

    print("----------------------------------------------------------------")
    print("{:52s} = {:<10.3f}".format(status, value))

  # Now it's time for JSON
  if args.json:
    print(json.dumps({"scores": scores, "sum": value}, sort_keys=True, indent=4, separators=(',', ': ')))

  # Check for debug condition
  if args.plot:

    # If we have a plot file, calculate the two PNG filenames
    plot_file_diff = None
    plot_file_summ = None
    if args.plot_file != None:
      plot_file_diff = "{}-diff.png".format(args.plot_file)
      plot_file_summ = "{}-summ.png".format(args.plot_file)

    # Display/Save the plots
    displayPlot(dataset_input, dataset_reference, dataset_input[0].axes[0],
                dataset_input[0].metrics, pf_plotDiff, plot_file_diff)
    displayPlot(dataset_input, dataset_reference, dataset_input[0].axes[0],
                dataset_input[0].metrics, pf_plotSumm, plot_file_summ)

  # Check if score is within threshold
  if value >= args.threshold:
    sys.exit(2)
  else:
    sys.exit(0)
